{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c617ff-33b6-4364-97a9-3e874a87800d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "‚ú® Welcome to the AI Story Generator! ‚ú®\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "What should your story be about? (e.g., 'A young wizard finds a magic book'):  In a kingdom ruled by dragons, a blacksmith's apprentice forges a weapon powerful enough to challenge the beasts.\n",
      "\n",
      "How many words should the story be (approx.)? (e.g., 100):  300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåü Generating your story... Please wait! üåü\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "MODEL_NAME = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Set pad token to avoid attention mask issues\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load a lightweight SentenceTransformer model for semantic similarity\n",
    "semantic_model = SentenceTransformer(\"paraphrase-MiniLM-L3-v2\")\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "def generate_story_chunk(prompt, max_new_tokens=50, temperature=0.6, top_p=0.8):\n",
    "    \"\"\"\n",
    "    Generates a chunk of the story based on the given prompt (optimized parameters).\n",
    "    \"\"\"\n",
    "    input_data = tokenizer.encode_plus(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=256,  # Reduce input size\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "    input_ids = input_data[\"input_ids\"].to(device)\n",
    "    attention_mask = input_data[\"attention_mask\"].to(device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,  # Lower temperature for faster convergence\n",
    "        top_p=top_p,  # Less diversity\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.3,\n",
    "        no_repeat_ngram_size=3,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "\n",
    "\n",
    "def remove_repetition(text, all_previous_chunks, similarity_threshold=0.85):\n",
    "    \"\"\"\n",
    "    Removes repetitive sentences based on semantic similarity, checking against previous chunks.\n",
    "    \"\"\"\n",
    "    sentences = text.split(\". \")\n",
    "    filtered_sentences = []\n",
    "    embeddings = semantic_model.encode(sentences, convert_to_tensor=True)  # Batch encode\n",
    "\n",
    "    for i, sentence_embedding in enumerate(embeddings):\n",
    "        is_similar = False\n",
    "        # Check for repetition within the current chunk\n",
    "        for prev in filtered_sentences:\n",
    "            prev_embedding = semantic_model.encode(prev, convert_to_tensor=True)\n",
    "            similarity = util.cos_sim(sentence_embedding, prev_embedding).item()\n",
    "            if similarity > similarity_threshold:\n",
    "                is_similar = True\n",
    "                break\n",
    "\n",
    "        # Check for repetition against previously added chunks\n",
    "        if not is_similar:\n",
    "            for prev_chunk in all_previous_chunks:\n",
    "                chunk_embedding = semantic_model.encode(prev_chunk, convert_to_tensor=True)\n",
    "                similarity = util.cos_sim(sentence_embedding, chunk_embedding).item()\n",
    "                if similarity > similarity_threshold:\n",
    "                    is_similar = True\n",
    "                    break\n",
    "        \n",
    "        if not is_similar:\n",
    "            filtered_sentences.append(sentences[i].strip())\n",
    "\n",
    "    return \". \".join(filtered_sentences).strip()\n",
    "\n",
    "\n",
    "def clean_story_ending(text):\n",
    "    \"\"\"\n",
    "    Cleans the story ending by removing incomplete trailing sentences.\n",
    "    \"\"\"\n",
    "    sentences = text.split(\". \")\n",
    "    if sentences[-1].endswith(\".\") or len(sentences) < 2:\n",
    "        return text.strip()\n",
    "    return \". \".join(sentences[:-1]) + \".\"\n",
    "\n",
    "\n",
    "def validate_chunk(chunk, current_story, similarity_threshold=0.75):\n",
    "    \"\"\"\n",
    "    Validates a generated chunk to ensure it adds meaningful content to the story.\n",
    "    \"\"\"\n",
    "    current_story_embedding = semantic_model.encode(current_story, convert_to_tensor=True)\n",
    "    chunk_embedding = semantic_model.encode(chunk, convert_to_tensor=True)\n",
    "    similarity = util.cos_sim(current_story_embedding, chunk_embedding).item()\n",
    "    return similarity < similarity_threshold\n",
    "\n",
    "\n",
    "def count_words(text):\n",
    "    \"\"\"\n",
    "    Counts the number of words in a given text.\n",
    "    \"\"\"\n",
    "    return len(text.split())\n",
    "\n",
    "\n",
    "def build_story(prompt, word_count=150):\n",
    "    \"\"\"\n",
    "    Builds the story in chunks to match the target word count, handling repetition more effectively.\n",
    "    \"\"\"\n",
    "    max_new_tokens = 40  # Reduced for faster iterations\n",
    "    current_story = prompt\n",
    "    all_previous_chunks = []  # List to keep track of all previous chunks\n",
    "    while count_words(current_story) < word_count:\n",
    "        remaining_words = word_count - count_words(current_story)\n",
    "        continuation_prompt = f\"{current_story.strip()} What happens next?\"\n",
    "        chunk = generate_story_chunk(continuation_prompt, max_new_tokens=min(max_new_tokens, remaining_words * 2))\n",
    "        chunk = remove_repetition(chunk, all_previous_chunks)  # Check for repetition across chunks\n",
    "        chunk = clean_story_ending(chunk)\n",
    "\n",
    "        # Skip validation every chunk\n",
    "        if count_words(chunk) > 5:\n",
    "            current_story += \" \" + chunk\n",
    "            all_previous_chunks.append(chunk)  # Save the new chunk to track repetition\n",
    "\n",
    "    return current_story.strip()\n",
    "\n",
    "\n",
    "def evaluate_story(prompt, story):\n",
    "    \"\"\"\n",
    "    Evaluates the semantic similarity between the prompt and story.\n",
    "    \"\"\"\n",
    "    prompt_embedding = semantic_model.encode(prompt, convert_to_tensor=True)\n",
    "    story_embedding = semantic_model.encode(story, convert_to_tensor=True)\n",
    "    similarity = util.cos_sim(prompt_embedding, story_embedding).item()\n",
    "    return similarity * 100\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to handle user input and generate a story.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"‚ú® Welcome to the AI Story Generator! ‚ú®\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    prompt = input(\"\\nWhat should your story be about? (e.g., 'A young wizard finds a magic book'): \").strip()\n",
    "    if not prompt:\n",
    "        print(\"‚ö†Ô∏è Please enter a valid story idea.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        word_count = int(input(\"\\nHow many words should the story be (approx.)? (e.g., 100): \").strip())\n",
    "    except ValueError:\n",
    "        print(\"‚ö†Ô∏è Invalid input. Using default length of 150 words.\")\n",
    "        word_count = 150\n",
    "\n",
    "    print(\"\\nüåü Generating your story... Please wait! üåü\")\n",
    "    story = build_story(prompt, word_count=word_count)\n",
    "    relevance_score = evaluate_story(prompt, story)\n",
    "\n",
    "    print(\"\\nHere‚Äôs your story:\\n\")\n",
    "    print(\"=\" * 50)\n",
    "    print(story)\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"\\nüîç Relevance to prompt: {relevance_score:.2f}%\")\n",
    "    print(f\"üìä Word Count (Story): {count_words(story)}\")\n",
    "    print(\"\\nüåü Thank you for using the AI Story Generator! üåü\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaed9185-5685-49ff-a0d5-38cd998587a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
