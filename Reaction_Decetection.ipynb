{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2b1702-c565-48a5-8eff-c7a22b240072",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install opencv-python keras tensorflow matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40a058b-1c92-4545-972a-5593f5188862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Define the paths to the dataset\n",
    "train_data_dir = 'C:/Users/Admin/Desktop/reaction_pre-train_dataset/test'  # Replace with the actual path\n",
    "test_data_dir = 'C:/Users/Admin/Desktop/reaction_pre-train_dataset/train'  # Replace with the actual path\n",
    "\n",
    "# Define image dimensions and batch size\n",
    "img_size = 48\n",
    "batch_size = 64\n",
    "num_classes = 7  # 7 emotions: Angry, Disgust, Fear, Happy, Sad, Surprise, Neutral\n",
    "\n",
    "# Function to load data\n",
    "def load_data(data_dir):\n",
    "    categories = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for category in categories:\n",
    "        path = os.path.join(data_dir, category)\n",
    "        class_num = categories.index(category)  # Assigning labels for each category\n",
    "        \n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "                resized_img = cv2.resize(img_array, (img_size, img_size))\n",
    "                data.append(resized_img)\n",
    "                labels.append(class_num)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img}: {e}\")\n",
    "    \n",
    "    data = np.array(data).reshape(-1, img_size, img_size, 1)  # Reshaping to fit the model\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return data, labels\n",
    "\n",
    "# Load the training and testing data\n",
    "train_data, train_labels = load_data(train_data_dir)\n",
    "test_data, test_labels = load_data(test_data_dir)\n",
    "\n",
    "# Normalize the pixel values to range [0, 1]\n",
    "train_data = train_data / 255.0\n",
    "test_data = test_data / 255.0\n",
    "\n",
    "# One-hot encode the labels\n",
    "train_labels = to_categorical(train_labels, num_classes)\n",
    "test_labels = to_categorical(test_labels, num_classes)\n",
    "\n",
    "# Model architecture\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional layers\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(img_size, img_size, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten and fully connected layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    zoom_range=0.1,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    datagen.flow(train_data, train_labels, batch_size=batch_size),\n",
    "    epochs=25,\n",
    "    validation_data=(test_data, test_labels),\n",
    "    steps_per_epoch=len(train_data) // batch_size\n",
    ")\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_acc = model.evaluate(test_data, test_labels)\n",
    "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "\n",
    "# Save the model\n",
    "model.save('emotion_detection_model.h5')\n",
    "\n",
    "# Classification report\n",
    "predictions = model.predict(test_data)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "true_labels = np.argmax(test_labels, axis=1)\n",
    "\n",
    "print(classification_report(true_labels, predicted_labels, target_names=['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']))\n",
    "\n",
    "# Confusion matrix\n",
    "print(confusion_matrix(true_labels, predicted_labels))\n",
    "\n",
    "# Function to predict emotion from a new image\n",
    "def predict_emotion(image_path):\n",
    "    categories = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "    \n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.resize(img, (img_size, img_size))\n",
    "    img = img.reshape(1, img_size, img_size, 1)\n",
    "    img = img / 255.0\n",
    "    \n",
    "    prediction = model.predict(img)\n",
    "    max_index = np.argmax(prediction)\n",
    "    emotion = categories[max_index]\n",
    "    \n",
    "    print(f\"Predicted Emotion: {emotion}\")\n",
    "\n",
    "# Example usage\n",
    "predict_emotion('C:/Users/Admin/Desktop/Kids/4be1baf30d5f4c9132025ff7697dbc5b.jpg')  # Replace with the actual path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed43cd12-d24a-494a-9a0d-3aabf760f766",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c61c97-e11d-4c24-aff9-efa82893120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Define the paths to the dataset\n",
    "train_data_dir = 'C:/Users/Admin/Desktop/reaction_pre-train_dataset/train'  # Replace with the actual path\n",
    "test_data_dir = 'C:/Users/Admin/Desktop/reaction_pre-train_dataset/test'    # Replace with the actual path\n",
    "\n",
    "# Define image dimensions and batch size\n",
    "img_size = 48\n",
    "batch_size = 64\n",
    "num_classes = 7  # 7 emotions: Angry, Disgust, Fear, Happy, Sad, Surprise, Neutral\n",
    "\n",
    "# Data augmentation and preprocessing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,\n",
    "    zoom_range=0.1,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load data using flow_from_directory\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "# Model architecture\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional layers\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(img_size, img_size, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten and fully connected layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=25,\n",
    "    validation_data=test_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_steps=len(test_generator)\n",
    ")\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "\n",
    "# Save the model\n",
    "model.save('emotion_detection_model.h5')\n",
    "\n",
    "# Classification report\n",
    "predictions = model.predict(test_generator)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "true_labels = test_generator.classes\n",
    "\n",
    "print(classification_report(true_labels, predicted_labels, target_names=train_generator.class_indices.keys()))\n",
    "\n",
    "# Confusion matrix\n",
    "print(confusion_matrix(true_labels, predicted_labels))\n",
    "\n",
    "# Function to predict emotion from a new image\n",
    "def predict_emotion(image_path):\n",
    "    categories = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "    \n",
    "    img = tf.keras.preprocessing.image.load_img(image_path, color_mode=\"grayscale\", target_size=(img_size, img_size))\n",
    "    img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img = img.reshape(1, img_size, img_size, 1)\n",
    "    img = img / 255.0\n",
    "    \n",
    "    prediction = model.predict(img)\n",
    "    max_index = np.argmax(prediction)\n",
    "    emotion = categories[max_index]\n",
    "    \n",
    "    print(f\"Predicted Emotion: {emotion}\")\n",
    "\n",
    "# Example usage\n",
    "predict_emotion('C:/Users/Admin/Desktop/Kids/4be1baf30d5f4c9132025ff7697dbc5b.jpg')  # Replace with the actual path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4c8fb4-36bd-440e-bcf4-86d27536fab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed47d51-0527-4f4e-8070-d0b105415045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Define the paths to the dataset\n",
    "train_data_dir = 'C:/Users/Admin/Desktop/reaction_pre-train_dataset/train'  # Replace with the actual path\n",
    "test_data_dir = 'C:/Users/Admin/Desktop/reaction_pre-train_dataset/test'    # Replace with the actual path\n",
    "\n",
    "# Define image dimensions and batch size\n",
    "img_size = 48\n",
    "batch_size = 64\n",
    "num_classes = 7  # 7 emotions: Angry, Disgust, Fear, Happy, Sad, Surprise, Neutral\n",
    "\n",
    "# Data augmentation and preprocessing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,\n",
    "    zoom_range=0.1,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load data using flow_from_directory\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "# Model architecture\n",
    "model = Sequential()\n",
    "\n",
    "# Use an explicit Input layer for better control\n",
    "model.add(Input(shape=(img_size, img_size, 1)))\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten and fully connected layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Steps for training and validation\n",
    "steps_per_epoch = train_generator.samples // batch_size\n",
    "validation_steps = test_generator.samples // batch_size\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=25,\n",
    "    validation_data=test_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps\n",
    ")\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "\n",
    "# Save the model\n",
    "model.save('emotion_detection_model.h5')\n",
    "\n",
    "# Classification report and confusion matrix\n",
    "predictions = model.predict(test_generator)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "true_labels = test_generator.classes\n",
    "\n",
    "print(classification_report(true_labels, predicted_labels, target_names=list(train_generator.class_indices.keys())))\n",
    "print(confusion_matrix(true_labels, predicted_labels))\n",
    "\n",
    "# Function to predict emotion from a new image\n",
    "def predict_emotion(image_path):\n",
    "    categories = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "    \n",
    "    img = tf.keras.preprocessing.image.load_img(image_path, color_mode=\"grayscale\", target_size=(img_size, img_size))\n",
    "    img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img = img.reshape(1, img_size, img_size, 1)\n",
    "    img = img / 255.0\n",
    "    \n",
    "    prediction = model.predict(img)\n",
    "    max_index = np.argmax(prediction)\n",
    "    emotion = categories[max_index]\n",
    "    \n",
    "    print(f\"Predicted Emotion: {emotion}\")\n",
    "\n",
    "# Example usage\n",
    "predict_emotion('C:/Users/Admin/Desktop/Kids/4be1baf30d5f4c9132025ff7697dbc5b.jpg')  # Replace with the actual path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affd2310-e045-4593-a40e-0328208acef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000c4b86-10f2-44c0-9810-4da025be5181",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b0b89f-fb1d-4574-97ac-295e72bc4871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model('emotion_detection_model.h5')\n",
    "\n",
    "# Define image dimensions and categories\n",
    "img_size = 48\n",
    "categories = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "# Function to predict emotion from a new image\n",
    "def predict_emotion(image_path):\n",
    "    img = tf.keras.preprocessing.image.load_img(image_path, color_mode=\"grayscale\", target_size=(img_size, img_size))\n",
    "    img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img = img.reshape(1, img_size, img_size, 1)\n",
    "    img = img / 255.0\n",
    "    \n",
    "    prediction = model.predict(img)\n",
    "    max_index = np.argmax(prediction)\n",
    "    emotion = categories[max_index]\n",
    "    \n",
    "    print(f\"Predicted Emotion: {emotion}\")\n",
    "\n",
    "# Example usage for a new image\n",
    "image_path = 'C:/Users/Admin/Desktop/Kids/istockphoto-515683938-612x612.jpg'  # Replace with the actual path of the new image\n",
    "predict_emotion(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884d993f-35be-4472-adc7-4f93a6e6d4a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef0d136-f13d-4ed7-b17f-6231cdc84301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2  # OpenCV library for image processing\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model('emotion_detection_model.h5')\n",
    "\n",
    "# Define image dimensions and categories\n",
    "img_size = 48\n",
    "categories = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "# Function to predict emotion from a new image and display it with a green frame and text\n",
    "def predict_emotion(image_path):\n",
    "    # Load the image using OpenCV\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    # Resize the image for the model prediction (48x48 grayscale)\n",
    "    face = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "    face_resized = cv2.resize(face, (img_size, img_size))  # Resize the image to 48x48\n",
    "    face_resized = face_resized.reshape(1, img_size, img_size, 1)\n",
    "    face_resized = face_resized / 255.0  # Normalize the image\n",
    "\n",
    "    # Make a prediction\n",
    "    prediction = model.predict(face_resized)\n",
    "    max_index = np.argmax(prediction)\n",
    "    emotion = categories[max_index]\n",
    "    \n",
    "    # Display the emotion on the image and draw a green frame\n",
    "    color = (0, 255, 0)  # Green color for the frame and text\n",
    "    thickness = 2  # Thickness of the frame\n",
    "\n",
    "    # Draw a green rectangle around the image (simulating a frame)\n",
    "    h, w, _ = img.shape\n",
    "    cv2.rectangle(img, (10, 10), (w-10, h-10), color, thickness)\n",
    "\n",
    "    # Add the predicted emotion text on top of the image\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(img, emotion, (20, 50), font, 1, color, 2, cv2.LINE_AA)\n",
    "\n",
    "    # Display the resulting image with the green frame and emotion text\n",
    "    cv2.imshow(\"Emotion Prediction\", img)\n",
    "    \n",
    "    # Wait for a key press and close the window\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage for a new image\n",
    "image_path = 'C:/Users/Admin/Desktop/Kids/05-12-21-happy-people.jpg'  # Replace with the actual path of the new image\n",
    "predict_emotion(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3642ab68-320b-444a-8a84-64f1ddc35611",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ae4df4-8fa3-47b1-aa59-1c9ba7e864f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36fb2fe-2dc9-4dcd-b0d8-436bd8e55d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2  # OpenCV library for image processing\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model('emotion_detection_model.h5')\n",
    "\n",
    "# Define image dimensions and categories\n",
    "img_size = 48\n",
    "categories = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "# Function to predict emotion from a new image and save the output\n",
    "def predict_emotion(image_path, save_path):\n",
    "    # Load the image using OpenCV\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    # Resize the image for the model prediction (48x48 grayscale)\n",
    "    face = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "    face_resized = cv2.resize(face, (img_size, img_size))  # Resize the image to 48x48\n",
    "    face_resized = face_resized.reshape(1, img_size, img_size, 1)\n",
    "    face_resized = face_resized / 255.0  # Normalize the image\n",
    "\n",
    "    # Make a prediction\n",
    "    prediction = model.predict(face_resized)\n",
    "    max_index = np.argmax(prediction)\n",
    "    emotion = categories[max_index]\n",
    "    \n",
    "    # Display the emotion on the image and draw a green frame\n",
    "    color = (0, 255, 0)  # Green color for the frame and text\n",
    "    thickness = 2  # Thickness of the frame\n",
    "\n",
    "    # Draw a green rectangle around the image (simulating a frame)\n",
    "    h, w, _ = img.shape\n",
    "    cv2.rectangle(img, (10, 10), (w-10, h-10), color, thickness)\n",
    "\n",
    "    # Add the predicted emotion text on top of the image\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(img, emotion, (20, 50), font, 1, color, 2, cv2.LINE_AA)\n",
    "\n",
    "    # Save the resulting image with the green frame and emotion text\n",
    "    cv2.imwrite(save_path, img)\n",
    "\n",
    "    # Optionally, display the image (you can comment this out if you just want to save)\n",
    "    cv2.imshow(\"Emotion Prediction\", img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage for a new image\n",
    "image_path = 'C:/Users/Admin/Desktop/Kids/05-12-21-happy-people.jpg'  # Input image path\n",
    "save_path = 'C:/Users/Admin/Desktop/Kids/predicted_emotion_output.jpg'  # Output image save path\n",
    "predict_emotion(image_path, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d43fdf-0d13-4b62-9439-01f14019554c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a512ff-3acb-4b94-8168-14af9dd409c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907eacd9-6e89-45d2-aaf1-e049e8a2fa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "# Preprocess function for grayscale images\n",
    "def preprocess_image(image_path):\n",
    "    img_size = 48  # Assuming the model was trained on 48x48 images\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Read the image in grayscale\n",
    "    img_resized = cv2.resize(img, (img_size, img_size))  # Resize to 48x48\n",
    "    img_normalized = img_resized / 255.0  # Normalize pixel values (0-1)\n",
    "    img_reshaped = img_normalized.reshape(1, img_size, img_size, 1)  # Reshape for the model (1 channel for grayscale)\n",
    "    return img_reshaped\n",
    "\n",
    "# Paths to the two images\n",
    "image_paths = [\n",
    "    r'C:\\Users\\Admin\\Desktop\\Kids\\05-12-21-happy-people.jpg', \n",
    "    r'C:\\Users\\Admin\\Desktop\\Kids\\360_F_73243993_WfQ6CwwVbsSf36W0oLaCseTecG6dQMFm.jpg'\n",
    "]\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model('emotion_detection_model.h5')\n",
    "\n",
    "# Define the emotion categories (ensure these match your model's categories)\n",
    "categories = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "# Loop over each image, preprocess, and predict\n",
    "for image_path in image_paths:\n",
    "    image = preprocess_image(image_path)\n",
    "    prediction = model.predict(image)\n",
    "    predicted_class = np.argmax(prediction, axis=1)\n",
    "    \n",
    "    # Print the predicted emotion\n",
    "    print(f\"Image: {image_path} - Predicted Emotion: {categories[predicted_class[0]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833747e3-d9f5-4bde-9f1f-08028085e9f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876e916e-e389-4562-a13e-b8251ab97b23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06902ae9-7fe4-47bf-943a-46793e2b0d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 722ms/step\n",
      "Predicted Emotion: Angry\n",
      "Saved the image with emotion: C:\\Users\\Admin\\Desktop\\Kids\\girl-2961959_640_emotion.jpg\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2  # OpenCV library for image processing\n",
    "import os\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model('emotion_detection_model.h5')\n",
    "\n",
    "# Define image dimensions and categories\n",
    "img_size = 48\n",
    "categories = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "# Function to predict emotion from a new image and display it with a green frame and text\n",
    "def predict_emotion(image_path):\n",
    "    # Load the image using OpenCV\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    # Resize the image for the model prediction (48x48 grayscale)\n",
    "    face = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "    face_resized = cv2.resize(face, (img_size, img_size))  # Resize the image to 48x48\n",
    "    face_resized = face_resized.reshape(1, img_size, img_size, 1)\n",
    "    face_resized = face_resized / 255.0  # Normalize the image\n",
    "\n",
    "    # Make a prediction\n",
    "    prediction = model.predict(face_resized)\n",
    "    max_index = np.argmax(prediction)\n",
    "    emotion = categories[max_index]\n",
    "    \n",
    "    # Print the predicted emotion in the output\n",
    "    print(f\"Predicted Emotion: {emotion}\")\n",
    "    \n",
    "    # Display the emotion on the image and draw a green frame\n",
    "    color = (0, 255, 0)  # Green color for the frame and text\n",
    "    thickness = 2  # Thickness of the frame\n",
    "\n",
    "    # Draw a green rectangle around the image (simulating a frame)\n",
    "    h, w, _ = img.shape\n",
    "    cv2.rectangle(img, (10, 10), (w-10, h-10), color, thickness)\n",
    "\n",
    "    # Add the predicted emotion text on top of the image\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(img, emotion, (20, 50), font, 1, color, 2, cv2.LINE_AA)\n",
    "\n",
    "    # Save the resulting image with the green frame and emotion text\n",
    "    output_image_path = image_path.replace('.jpg', '_emotion.jpg')  # Modify the filename\n",
    "    cv2.imwrite(output_image_path, img)\n",
    "    print(f\"Saved the image with emotion: {output_image_path}\")\n",
    "\n",
    "    # Display the resulting image\n",
    "    cv2.imshow(\"Emotion Prediction\", img)\n",
    "    \n",
    "    # Wait for a key press and close the window\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage for a new image\n",
    "image_path = r'C:\\Users\\Admin\\Desktop\\Kids\\girl-2961959_640.jpg'  # Replace with the actual path of the new image\n",
    "predict_emotion(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fc8c74-9be7-45d9-82c1-58e40d602d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
