{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvMpBrLtEmi-"
      },
      "outputs": [],
      "source": [
        "# Install Hugging Face Transformers and PyTorch\n",
        "!pip install transformers torch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load the pre-trained conversational model (DialoGPT)\n",
        "model_name = \"microsoft/DialoGPT-medium\"  # You can also try 'small' or 'large'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Function to generate a chatbot response\n",
        "def generate_response(user_input):\n",
        "    # Tokenize input and move it to the device (GPU or CPU)\n",
        "    inputs = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Create an attention mask (all 1s since no padding)\n",
        "    attention_mask = torch.ones(inputs.shape, device=device)\n",
        "\n",
        "    # Generate response from the model\n",
        "    reply_ids = model.generate(\n",
        "        inputs,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=100,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        temperature=0.7,  # Controls randomness in generation\n",
        "        top_p=0.9,        # Nucleus sampling for more natural responses\n",
        "        do_sample=True    # Enable sampling for varied responses\n",
        "    )\n",
        "\n",
        "    # Decode the response\n",
        "    response = tokenizer.decode(reply_ids[:, inputs.shape[-1]:][0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Simple loop to chat with the bot\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Start chatting with the Bot (type 'quit' to stop)!\")\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == \"quit\":\n",
        "            break\n",
        "        bot_response = generate_response(user_input)\n",
        "        print(f\"Bot: {bot_response}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJyLWLkhGhDM",
        "outputId": "e6bc972d-5a6a-49f7-d9e1-b76cfec611bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start chatting with the Bot (type 'quit' to stop)!\n",
            "You: Hi\n",
            "Bot: Hi! How are you?\n",
            "You: quit\n"
          ]
        }
      ]
    }
  ]
}